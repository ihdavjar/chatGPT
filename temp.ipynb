{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/media/ihdav/Files/Research/Infrrd_Research/chatGPT/Data/Tiny_Shakespeare/input.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "distinct_char = sorted(list(set(text)))\n",
    "vocab_size = len(distinct_char)\n",
    "\n",
    "print(' '.join(distinct_char))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {c:i for i,c in enumerate(distinct_char)}\n",
    "itos = {i:c for i,c in enumerate(distinct_char)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda i: ''.join([itos[c] for c in i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 14, 56, 53]\n",
      "Hello Bro\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"Hello Bro\"))\n",
    "print(decode(encode(\"Hello Bro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text),dtype=torch.long)\n",
    "print(data.shape,data.dtype)\n",
    "print(data[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use 1 to 8 of these to predict the next one of the thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is :47\n",
      "when input is tensor([18, 47]) the target is :56\n",
      "when input is tensor([18, 47, 56]) the target is :57\n",
      "when input is tensor([18, 47, 56, 57]) the target is :58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is :1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is :15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is :47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is :58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[0:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is :{target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      " y=tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    \n",
    "    ix = torch.randint(len(data)-block_size,(batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x,y\n",
    "\n",
    "xb,yb = get_batch('train')\n",
    "print(f'x= {xb}\\n y={yb}')\n",
    "                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context tensor([24]) target: 43\n",
      "context tensor([24, 43]) target: 58\n",
      "context tensor([24, 43, 58]) target: 5\n",
      "context tensor([24, 43, 58,  5]) target: 57\n",
      "context tensor([24, 43, 58,  5, 57]) target: 1\n",
      "context tensor([24, 43, 58,  5, 57,  1]) target: 46\n",
      "context tensor([24, 43, 58,  5, 57,  1, 46]) target: 43\n",
      "context tensor([24, 43, 58,  5, 57,  1, 46, 43]) target: 39\n",
      "context tensor([44]) target: 53\n",
      "context tensor([44, 53]) target: 56\n",
      "context tensor([44, 53, 56]) target: 1\n",
      "context tensor([44, 53, 56,  1]) target: 58\n",
      "context tensor([44, 53, 56,  1, 58]) target: 46\n",
      "context tensor([44, 53, 56,  1, 58, 46]) target: 39\n",
      "context tensor([44, 53, 56,  1, 58, 46, 39]) target: 58\n",
      "context tensor([44, 53, 56,  1, 58, 46, 39, 58]) target: 1\n",
      "context tensor([52]) target: 58\n",
      "context tensor([52, 58]) target: 1\n",
      "context tensor([52, 58,  1]) target: 58\n",
      "context tensor([52, 58,  1, 58]) target: 46\n",
      "context tensor([52, 58,  1, 58, 46]) target: 39\n",
      "context tensor([52, 58,  1, 58, 46, 39]) target: 58\n",
      "context tensor([52, 58,  1, 58, 46, 39, 58]) target: 1\n",
      "context tensor([52, 58,  1, 58, 46, 39, 58,  1]) target: 46\n",
      "context tensor([25]) target: 17\n",
      "context tensor([25, 17]) target: 27\n",
      "context tensor([25, 17, 27]) target: 10\n",
      "context tensor([25, 17, 27, 10]) target: 0\n",
      "context tensor([25, 17, 27, 10,  0]) target: 21\n",
      "context tensor([25, 17, 27, 10,  0, 21]) target: 1\n",
      "context tensor([25, 17, 27, 10,  0, 21,  1]) target: 54\n",
      "context tensor([25, 17, 27, 10,  0, 21,  1, 54]) target: 39\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch_size):\n",
    "    for j in range(block_size):\n",
    "        print(f'context {xb[i,:j+1]} target: {yb[i,j]}')\n",
    "        # print(f'target {yb[i,j]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "    \n",
    "    def forward(self,idx,targets):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 65])\n",
      "tensor([[-1.5101, -0.0948,  1.0927,  0.1505,  1.6347, -0.0518,  0.4996,  0.7216,\n",
      "         -0.8968, -0.4122,  1.0030,  0.8508,  0.2178,  0.0328, -0.1699,  1.0659,\n",
      "         -0.6177,  1.1824,  0.0214, -0.2154, -1.4623,  2.1707,  0.1624,  1.0296,\n",
      "          0.4154,  0.6207,  0.2341, -0.0326,  1.0124,  1.5122, -0.3359,  0.2456,\n",
      "          1.8682,  0.7536, -0.1177, -0.1967, -0.9552, -0.8995, -0.9583, -0.5945,\n",
      "          0.1321, -0.5406,  0.1405, -0.7321,  1.1796,  1.3316, -0.2094,  0.0960,\n",
      "          0.9040, -0.4032,  0.3027, -0.8034, -1.2537, -1.5195,  0.7446,  1.1914,\n",
      "         -0.8061, -0.6290,  1.2447, -2.4400,  0.8408, -0.3993, -0.6126, -0.6597,\n",
      "          0.7624],\n",
      "        [ 0.3323, -0.0872, -0.7470, -0.6074,  0.3418,  0.5343,  0.3957, -0.4919,\n",
      "         -0.0894, -1.3886,  1.2835, -0.3975,  2.0152,  1.6773, -0.3833,  1.5728,\n",
      "          1.9458,  0.7247, -0.4834, -0.3263,  0.3193, -0.4198, -0.6435, -0.3311,\n",
      "          0.7554, -1.2385,  0.4067,  0.9982, -0.6511,  1.2450,  0.2804,  0.8371,\n",
      "         -0.4119,  0.2115, -0.6240,  0.0203, -0.3418,  1.4934,  1.7307,  1.3354,\n",
      "         -0.2712,  0.4902,  0.6600, -1.6321, -0.7858,  1.7688,  2.6160, -0.5767,\n",
      "         -0.3628, -2.7428,  0.7428,  0.0737,  0.2050, -0.5497,  2.1261, -0.9240,\n",
      "          0.1048,  0.8324,  1.4287, -0.7789,  2.9275, -0.8525, -0.6716, -0.9572,\n",
      "         -0.9594],\n",
      "        [ 0.2475, -0.6349, -1.2909,  1.1822,  0.1479, -0.4333, -0.8269,  0.0728,\n",
      "         -1.2982,  0.3960, -1.2460,  0.1458, -0.5699, -1.3561, -0.3812, -0.8515,\n",
      "          1.1918, -0.8108, -0.1733, -0.4703, -0.6000, -1.3636, -1.0889,  1.0108,\n",
      "          0.8543, -0.0441,  1.8017,  0.6014, -2.5448, -0.4865,  2.6412,  1.6053,\n",
      "          0.5901,  0.8137, -0.1124, -0.3050,  1.1426,  0.6637, -0.7000,  0.9262,\n",
      "         -1.1032, -1.2125,  0.6065,  0.5882, -0.5453,  0.7654,  0.5692,  0.8859,\n",
      "         -0.0700,  0.6792, -0.0283, -1.2243, -1.7192,  1.4801,  0.9587, -0.0338,\n",
      "          0.5083, -0.2502,  2.0734, -0.2994,  0.0473, -0.9626,  1.3064, -0.2256,\n",
      "         -1.8305],\n",
      "        [-0.1338,  0.3899, -0.2884, -1.4651,  0.0101, -0.3004, -1.5733,  0.0148,\n",
      "         -0.0447, -0.5367, -0.5223, -0.2181, -2.1608,  0.7865,  0.6854, -1.2576,\n",
      "          0.6094, -2.0551, -0.4431, -0.6499, -0.6870,  0.2567, -1.2669,  0.2645,\n",
      "         -0.6445,  1.0834, -0.7995,  0.2922,  1.3143,  1.2607, -0.3505, -2.0660,\n",
      "          1.0575, -1.0572,  0.9911, -0.0797,  1.0751,  0.2381,  0.5757,  1.6685,\n",
      "          0.5976, -1.8736,  1.2910, -0.3753, -1.8943,  0.5557,  0.8567, -0.8461,\n",
      "          0.5015, -0.9656, -0.7255,  0.0990,  0.5928, -0.0422, -0.9566,  1.4424,\n",
      "          0.4341, -0.4292,  0.3666,  0.1275, -0.0560,  0.8315, -0.5512,  1.0477,\n",
      "          1.6187],\n",
      "        [-0.5201,  0.2831,  1.0847,  1.9905,  0.7763, -0.8460,  0.8437,  0.7905,\n",
      "         -0.5287, -0.1187,  0.6618, -0.6682, -1.8731,  0.7459,  2.1471,  1.0535,\n",
      "         -0.7480,  2.0704, -1.1879, -0.7858,  0.1276, -0.9183,  0.5782, -1.7134,\n",
      "         -1.2302, -0.4149, -0.9652, -0.9685, -0.2536, -1.0255, -0.9492, -0.1503,\n",
      "          0.4905, -1.1986,  1.0955, -0.5802,  0.0199, -2.0645, -0.0617, -0.4054,\n",
      "         -0.7169,  0.9026, -0.3288, -0.2391, -1.0618, -0.1223, -1.4403,  0.8433,\n",
      "         -0.7001,  0.9611,  0.8550,  0.4062, -2.2157, -0.3732, -0.6900,  0.4235,\n",
      "          2.6768,  1.0813,  0.6548,  1.9577,  0.1433, -0.0627, -0.0198,  0.7959,\n",
      "          1.6014],\n",
      "        [ 0.5978, -0.0514, -0.0646, -0.4970,  0.4658, -0.2573, -1.0673,  2.0089,\n",
      "         -0.5370,  0.2228,  0.6971, -1.4267,  0.9059,  0.1446,  0.2280,  2.4900,\n",
      "         -1.2237,  1.0107,  0.5560, -1.5935, -1.2706,  0.6903, -0.1961,  0.3449,\n",
      "         -0.3419,  0.4759, -0.7663, -0.4190, -0.4370, -1.0012, -0.4094, -1.6669,\n",
      "         -1.3651, -0.1655,  0.9623,  0.0315, -0.7419, -0.2978,  0.0172, -0.1772,\n",
      "         -0.1334,  0.2940,  1.3850,  0.1209,  2.5418, -0.6405, -1.9740, -0.3296,\n",
      "          0.0080,  0.9262, -1.8846,  0.1670,  0.4586, -1.7662,  0.5860,  1.7510,\n",
      "          0.2807,  0.3110, -0.6538, -0.6576,  0.3184, -0.5496, -1.4649, -2.0555,\n",
      "          1.8275],\n",
      "        [ 1.0901,  0.2170, -2.9996,  1.4690, -0.1948, -0.1507,  0.2601, -0.9647,\n",
      "          0.1162, -0.8295, -0.2266,  0.0219, -0.2785, -0.4851, -1.8023, -0.7330,\n",
      "         -1.2828,  0.8863,  1.0515, -0.9823, -1.6369, -1.3499,  0.1830,  0.0532,\n",
      "         -1.1438, -0.2829, -0.5979,  1.4757,  0.4655, -3.0346,  0.5516,  1.3107,\n",
      "          0.1240, -1.8046,  0.2700, -0.4322,  0.2784, -0.5599,  1.2502,  0.7051,\n",
      "         -1.0169,  0.4854, -1.0808, -0.3128, -0.4189, -0.5718,  0.8215,  1.7384,\n",
      "          0.5578,  0.6167,  1.5260, -0.3508, -1.5615,  0.4548, -0.8935,  0.3642,\n",
      "          0.5714,  2.7072, -1.5443,  1.1288, -1.1217, -1.7328, -0.5472, -0.8017,\n",
      "          0.7761],\n",
      "        [ 0.3323, -0.0872, -0.7470, -0.6074,  0.3418,  0.5343,  0.3957, -0.4919,\n",
      "         -0.0894, -1.3886,  1.2835, -0.3975,  2.0152,  1.6773, -0.3833,  1.5728,\n",
      "          1.9458,  0.7247, -0.4834, -0.3263,  0.3193, -0.4198, -0.6435, -0.3311,\n",
      "          0.7554, -1.2385,  0.4067,  0.9982, -0.6511,  1.2450,  0.2804,  0.8371,\n",
      "         -0.4119,  0.2115, -0.6240,  0.0203, -0.3418,  1.4934,  1.7307,  1.3354,\n",
      "         -0.2712,  0.4902,  0.6600, -1.6321, -0.7858,  1.7688,  2.6160, -0.5767,\n",
      "         -0.3628, -2.7428,  0.7428,  0.0737,  0.2050, -0.5497,  2.1261, -0.9240,\n",
      "          0.1048,  0.8324,  1.4287, -0.7789,  2.9275, -0.8525, -0.6716, -0.9572,\n",
      "         -0.9594]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb[0,:],yb[0,:])\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titans_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
